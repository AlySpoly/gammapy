#!/usr/bin/env python
"""An iterative multi-scale source detection method.

This is a prototype implementation of the following algorithm:
1. Input is: count, background and exposure map and list of scales
2. Compute significance maps on multiple scales (tophat correlate)
3. Largest peak on any scale gives a seed position / extension (the scale)
4. Fit a 2D Gauss-model source using the seed parameters
5. Add the source to a list of detected sources and the background model
6. Restart at 2, but this time with detected sources added to the background
   model, i.e. significance maps will be "residual significance" maps.

TODO: tons of things, e.g.
* Use TS maps with Gauss source morphology instead of tophat.
* Add PSF convolution
* Use astropy.modeling or Sherpa
* Make it more modular and more abstract; put in tevpy.detect
  - user should be able to plug-in their significance map computation?
  - support different source models?
  - Separate Iterator, SignificanceMapCalculator, Guesser, Fitter ...
    and implement different methods as sub-classes or via callbacks?
  - e.g. list of peaks should be replaced with some abstract class that
    allows different characteristics / methods to be implemented.
* Introduce parameters that allow us to vary the procedure
* Should the IterativeSourceDetector do the I/O and pixel-world coordinate conversions
  or be astro-independent?
* Add possibilities to write dump debug maps (e.g. residual significance maps)
  and info (e.g. peak positions).
* Check if Python garbage collection for iter_maps sets in OK
  or if explicit garbage collection is needed.
* Use photutils aperture photometry for estimate_flux?
* Introduce FLUX_SCALE = 1e-10 parameter to avoid roundoff error problems?
"""
import os
import logging
logging.basicConfig(level=logging.DEBUG, format='%(levelname)s - %(message)s')
import numpy as np
from astropy.io import fits
from tevpy import stats
from tevpy.utils.image import tophat_correlate

class FitFailedError:
    pass

def gauss2d(x, y, xpos, ypos, sigma, flux):
    """2D Gaussian source model"""
    x = np.asanyarray(x, dtype=np.float64)
    y = np.asanyarray(y, dtype=np.float64)
    theta2 = (x - xpos) ** 2 + (y - ypos) ** 2
    sigma2 = sigma * sigma
    term_a = 1 / (2 * np.pi * sigma2)
    term_b = np.exp(-0.5 * theta2 / sigma2)
    image = term_a * term_b
    return flux * image / image.sum()


class IterativeSourceDetector(object):
    """TODO: document me
    """

    def __init__(self, maps, scales,max_sources=10, significance_threshold=5,
                 max_ncall=300, debug_output_folder=''):
        """
        debug_output_folder: empty string for no debug output
        """
        
        self.maps = maps
        # Note: FITS convention is to start counting pixels at 1
        y, x = np.indices(maps['counts'].shape, dtype=np.int32) + 1
        self.maps['x'], self.maps['y'] = x, y

        # Temp maps that change in each iteration
        self.iter_maps = dict()
        
        self.scales = np.asanyarray(scales)
        self.max_sources = max_sources
        self.significance_threshold = significance_threshold
        self.max_ncall = max_ncall
        self.debug_output_folder = debug_output_folder

        self.sources_guess = []        
        self.sources = []
        
        # At the moment we only
        #self.peaks = np.zeros_like(self.scales)

    def run(self):
        logging.debug('Running source detection')

        for _ in range(self.max_sources):
            logging.debug('Starting iteration number {0}'.format(_))
            debug_folder = self.debug_output_folder + '/' + str(_)
            if self.debug_output_folder:
                try:
                    os.mkdir(debug_folder)
                    logging.info('mkdir {0}'.format(debug_folder))
                except:
                    logging.debug('Folder exists: {0}'.format(debug_folder))

            self.compute_iter_maps()
            if self.debug_output_folder:
                # Save per iteration maps
                for name in ['background']:
                    filename = '{0}/{1}.fits'.format(debug_folder, name)
                    logging.info('Writing {0}'.format(filename))
                    fits.writeto(filename, self.iter_maps[name])
                
                # Save per iteration and scale maps
                for name in ['significance']:
                    for scale in self.scales:
                        filename = '{0}/{1}_{2}.fits'.format(debug_folder, name, scale)
                        logging.info('Writing {0}'.format(filename))
                        fits.writeto(filename, self.iter_maps[name][scale])
                
            self.find_peaks()
            # TODO: debug output to JSON here and for later steps

            if self.stop_iteration():
                break

            self.guess_source_parameters()
            if self.debug_output_folder:
                filename = '{0}/{1}'.format(debug_folder, 'sources_guess.reg')
                self.save_regions(filename, selection='guess')

            try:
                self.fit_source_parameters()
            except FitFailedError:
                logging.warning('Fit failed. Full stop.')
                break

    def compute_iter_maps(self):
        logging.debug('Computing maps for this iteration.')
        self.iter_maps = dict()
        
        background = self.maps['background']
        background += self.model_excess(self.sources)
        self.iter_maps['background'] = background

        self.iter_maps['significance'] = dict()
        for scale in self.scales:
            counts = tophat_correlate(self.maps['counts'], scale)
            background = tophat_correlate(self.iter_maps['background'], scale)
            significance = stats.significance(counts, background)
            self.iter_maps['significance'][scale] = significance

    def model_excess(self, sources):
        # logging.debug('Computing model excess')
        x, y = self.maps['x'], self.maps['y']
        flux = np.zeros_like(x, dtype=np.float64)
        for source in sources:
            # logging.debug('Adding source: {0}'.format(source))
            source_flux = gauss2d(x, y, **source)
            # logging.debug('Source flux: {0}'.format(source_flux.sum()))
            flux += source_flux
            # logging.debug('Total flux: {0}'.format(flux.sum()))
        excess = flux * self.maps['exposure']
        return excess

    def find_peaks(self):
        logging.debug('Finding peaks.')
        self.peaks = []
        for scale in self.scales:
            image = self.iter_maps['significance'][scale]
            # Note: significance images sometimes contain Inf or NaN values.
            # We set them here to a value so that they will be ignored
            mask = np.invert(np.isfinite(image))
            image[mask] = -1e10

            # This is how to find the peak position in a 2D numpy array
            y, x = np.unravel_index(np.nanargmax(image), image.shape)
            val = image[y, x]
            peak = dict()
            peak['xpos'], peak['ypos'] = x, y
            peak['val'], peak['scale'] = val, scale
            self.peaks.append(peak)
            logging.debug('Peak on scale {scale:5.2f} is at ({xpos:5d}, {ypos:5d}) with value {val:7.2f}'
                          ''.format(**peak))
    
    def stop_iteration(self):
        """Criteria to stop the iteration process"""
        max_significance = max([_['val'] for _ in self.peaks])
        if max_significance < self.significance_threshold:
            logging.debug('Max peak significance of {0:7.2f} is smaller than detection threshold {1:7.2f}'
                          ''.format(max_significance, self.significance_threshold))
            logging.debug('Stopping iteration.')
            return True
        else:
            return False

    def guess_source_parameters(self):
        """Guess source start parameters for the fit.
        
        At the moment take the position and scale of the maximum residual peak
        and compute the excess within a circle around that position.
        """
        logging.debug('Guessing Gauss source parameters:')

        # Find the scale with the most significant peak
        peak = self.peaks[0]
        for _ in range(1, len(self.scales)):
            if self.peaks[_]['val'] > peak['val']:
                peak = self.peaks[_]

        source = dict()
        source['xpos'], source['ypos'] = peak['xpos'], peak['ypos']
        # TODO: introduce rough scale factor tophat -> gauss here
        SIGMA_SCALE_FACTOR = 1
        source['sigma'] = SIGMA_SCALE_FACTOR * peak['scale']
        logging.debug('xpos: {xpos}'.format(**source))
        logging.debug('ypos: {ypos}'.format(**source))
        logging.debug('sigma: {sigma}'.format(**source))
        source['flux'] = self.estimate_flux(source)
        self.sources_guess.append(source)

    def fit_source_parameters(self):
        """Fit source parameters using the guess as start values

        For this prototype we simply roll our own using iminuit,
        this should probably be changed to astropy or Sherpa.
        """
        logging.debug('Fitting source parameters')
        from iminuit import Minuit
        def fit_stat(xpos, ypos, sigma, flux):
            """Define CASH fit statistic for Gauss model"""
            data = self.maps['counts']
            # Note: No need to re-compute excess model for all previous source,
            # that is already contained in the background in iter_maps.
            background = self.iter_maps['background']
            sources = [dict(xpos=xpos, ypos=ypos, sigma=sigma, flux=flux)]
            model = background + self.model_excess(sources)
            cash =  stats.cash(data, model).sum()
            return cash

        source = self.sources_guess[-1]
        logging.debug('Source parameters before fit: {0}'.format(source))
        #import IPython; IPython.embed(); 1 / 0       
        minuit = Minuit(fit_stat, pedantic=False, print_level=0, **source)
        minuit.migrad(ncall=self.max_ncall)
        # If fit doesn't converge we simply abort
        # TODO: should we use exceptions here or return False as signal?
        if not minuit.migrad_ok():
            raise FitFailedError
        
        # Store best-fit source parameters
        source = minuit.values
        logging.debug('Source parameters  after fit: {0}'.format(source))

    def estimate_flux(self, source):
        """Estimate flux in a circular region around the source"""
        logging.debug('Estimating flux')
        SOURCE_RADIUS_FACTOR = 2
        radius = SOURCE_RADIUS_FACTOR * source['sigma']
        r2 = ((self.maps['x'] - source['xpos']) ** 2 +
              (self.maps['y'] - source['ypos']) ** 2)
        mask = (r2 < radius ** 2)
        npix = mask.sum()
        counts = self.maps['counts'][mask].sum()
        background = self.iter_maps['background'][mask].sum()
        exposure = self.maps['exposure'][mask].sum()
        excess = counts - background
        flux = excess / exposure
        logging.debug('Flux estimation for source region radius: {0}'.format(radius))
        logging.debug('npix: {0}'.format(npix))
        logging.debug('counts: {0}'.format(counts))
        logging.debug('background: {0}'.format(background))
        logging.debug('excess: {0}'.format(excess))
        logging.debug('exposure: {0}'.format(exposure))
        logging.debug('flux: {0}'.format(flux))
        return flux 

    def save_fits(self, filename):
        logging.info('Writing source detections in FITS format to {0}'.format(filename))
        # TODO

    def save_regions(self, filename, selection='fit'):
        logging.info('Writing source detections in ds9 region format to {0}'.format(filename))
        if selection == 'fit':
            sources = self.sources
            color = 'green'
        elif selection == 'guess':
            sources = self.sources_guess
            color = 'magenta'
        else:
            raise ValueError('Unknown selection: {0}'.format(selection))
        with open(filename, 'w') as outfile:
            outfile.write('image\n')
            for ii, source in enumerate(sources):
                fmt = 'circle({xpos:3.3f},{ypos:3.3f},{radius:3.3f}) # text="{name}" color={color}\n'
                data = dict(xpos=source['xpos'], ypos=source['ypos'])
                N_SIGMA = 3
                data['radius'] = N_SIGMA * source['sigma']
                data['name'] = 'Source {0}'.format(ii)
                data['color'] = color
                text = fmt.format(**data)
                outfile.write(text)

    def save_json(self, filename):
        logging.info('Writing source detections in JSON format to {0}'.format(filename))
        import json
        data = dict(sources=self.sources, sources_guess=self.sources_guess)
        #print data
        #import IPython; IPython.embed(); 1/0
        # TODO: this fails because data contains np.float32 values, which are not JSON serializable:
        # TypeError: 1.2617354e-10 is not JSON serializable 
        with open(filename, 'w') as outfile:
            json.dump(data, outfile, indent=4)

if __name__ == '__main__':

    # Parse command line arguments
    import argparse
    parser = argparse.ArgumentParser(description='A prototype iterative source detection tool')
    parser.add_argument('scales', type=float, nargs='*', default=[0.1, 0.2, 0.4],
                        help='List of spatial scales (deg) to search for sources'
                        '[default=%(default)s]')
    parser.add_argument('--counts', type=str, default='counts.fits',
                        help='Counts FITS file name '
                        '[default=%(default)s]')
    parser.add_argument('--background', type=str, default='background.fits',
                        help='Background FITS file name '
                        '[default=%(default)s]')
    parser.add_argument('--exposure', type=str, default='exposure.fits',
                        help='Exposure FITS file name '
                        '[default=%(default)s]')
    parser.add_argument('--output_fits', type=str, default='detections.fits',
                        help='Output catalog of detections (FITS table format) '
                        '[default=%(default)s]')
    parser.add_argument('--output_regions', type=str, default='detections.reg',
                        help='Output catalog of detections (ds9 region file format) '
                        '[default=%(default)s]')
    parser.add_argument('--debug_output_folder', type=str, default='',
                        help='Debug output folder name (empty string for no output) '
                        '[default=%(default)s]')
    args = parser.parse_args()
    args = vars(args)

    # Load data
    maps = dict()
    for mapname in ['counts', 'background', 'exposure']:
        filename = args[mapname]
        logging.info('Reading {0} map: {1}'.format(mapname, filename))
        maps[mapname] = fits.getdata(filename)
    
    # Compute scales in pixel coordinates
    DEG_PER_PIX = np.abs(fits.getval(args['counts'], 'CDELT1'))
    scales_deg = args['scales']
    scales_pix = np.array(scales_deg) / DEG_PER_PIX
    logging.info('Number of scales: {0}'.format(len(scales_deg)))
    logging.info('DEG_PER_PIX: {0}'.format(DEG_PER_PIX))
    logging.info('Scales in deg: {0}'.format(scales_deg))
    logging.info('Scales in pix: {0}'.format(scales_pix))

    # Run the iterative source detection
    detector = IterativeSourceDetector(maps=maps, scales=scales_pix,
                                       debug_output_folder=args['debug_output_folder'])
    detector.run()

    # Save the results
    #detector.save_fits(args['output_fits'])
    detector.save_regions(args['output_regions'])
    #detector.save_json('detect.json')
